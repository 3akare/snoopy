<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Report: A Multi-Modal Sign Language Recognition System</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- 
    Chosen Palette: "Warm Neutral Harmony" - A calming and professional palette.
    - Background: #FDFBF8 (Warm Off-White)
    - Main Text: #2D3748 (Charcoal)
    - Primary Accent: #4A5568 (Slate Gray)
    - Secondary Accent / Highlights: #4299E1 (Muted Blue)
    - Chart Colors: A curated, accessible color scheme.
    -->
    <!-- 
    Application Structure Plan: A single-page, vertical-scrolling dashboard with a sticky navigation bar. This structure was chosen to guide the user through a logical narrative—from the high-level problem to the detailed solution and results—while allowing non-linear exploration. The flow is:
    1.  **Overview:** Introduce the project and its significance with key top-line results.
    2.  **System Architecture:** Interactively break down the complex system into understandable modules. This is more engaging than a static diagram.
    3.  **Data & Features:** Detail the foundational dataset and the multi-modal features, which are crucial for understanding the model's input.
    4.  **Model & Training:** Focus on the "brain" of the system (the Bi-LSTM model) and its learning process.
    5.  **Evaluation & Results:** Present the performance metrics in a clear, digestible format with interactive visualizations.
    6.  **Simulated Demo:** Provide a "wow" factor by simulating the system's end-to-end functionality, making the abstract process concrete for the user.
    7.  **Conclusion:** Summarize the project's impact and future path.
    This structure transforms the linear research paper into an explorable, user-centric experience.
    -->
    <!-- 
    Visualization & Content Choices: 
    - Report Info: System Architecture -> Goal: Explain the end-to-end pipeline -> Viz/Presentation: Interactive HTML/CSS flow diagram -> Interaction: Click on a component to show detailed text in a side panel. -> Justification: More engaging and less overwhelming than a static image or a long wall of text. It encourages active exploration. -> Library/Method: HTML/Tailwind/JS.
    - Report Info: Multi-modal Feature Vector (1581 dimensions) -> Goal: Show the composition of the input data -> Viz/Presentation: Chart.js Donut Chart -> Interaction: Hover to see exact feature counts for Face, Pose, and Hands. -> Justification: Visually represents the data's composition much more effectively than text alone. -> Library/Method: Chart.js.
    - Report Info: Model Training Process -> Goal: Illustrate model learning and convergence -> Viz/Presentation: Chart.js Line Chart (Accuracy & Loss) -> Interaction: Hover over points to see epoch-specific values; toggle between accuracy/loss views. -> Justification: Standard and effective way to visualize the dynamics of model training. -> Library/Method: Chart.js.
    - Report Info: Class-specific Performance -> Goal: Show where the model performs well and where it gets confused -> Viz/Presentation: Interactive HTML/CSS Grid for Confusion Matrix (illustrative subset) -> Interaction: Hover over a cell to highlight it and see a descriptive tooltip (e.g., "Model incorrectly classified 'Brother' as 'Sister' 5 times."). -> Justification: An interactive matrix is far more user-friendly and insightful than a large, static image, especially for a large number of classes. A subset is used to maintain clarity. -> Library/Method: HTML/Tailwind/JS.
    - Report Info: End-to-end translation process -> Goal: Demonstrate the system's full capability in a tangible way -> Viz/Presentation: Simulated text generation sequence -> Interaction: User clicks "Run Simulation" to see the step-by-step translation process unfold. -> Justification: Provides a powerful, narrative-driven demonstration of the system's purpose and output. -> Library/Method: JS timeouts for animation.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->

    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #FDFBF8;
            color: #2D3748;
        }
        .nav-link {
            transition: color 0.3s ease, border-color 0.3s ease;
            border-bottom: 2px solid transparent;
        }
        .nav-link:hover, .nav-link.active {
            color: #4299E1;
            border-bottom-color: #4299E1;
        }
        .stat-card {
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        .arch-block {
            cursor: pointer;
            transition: background-color 0.3s ease, border-color 0.3s ease, color 0.3s ease;
        }
        .arch-block.active {
            background-color: #4299E1;
            border-color: #2B6CB0;
            color: white;
        }
        .arch-arrow {
            font-size: 2rem;
            color: #A0AEC0;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 400px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 350px;
            }
        }
    </style>
</head>
<body class="antialiased">

    <!-- Header & Navigation -->
    <header class="bg-white/80 backdrop-blur-md sticky top-0 z-50 shadow-sm">
        <nav class="container mx-auto px-6 py-3 flex justify-between items-center">
            <div class="text-xl font-bold text-gray-800">SLR System</div>
            <div class="hidden md:flex space-x-8">
                <a href="#overview" class="nav-link">Overview</a>
                <a href="#architecture" class="nav-link">Architecture</a>
                <a href="#data" class="nav-link">Data</a>
                <a href="#model" class="nav-link">Model</a>
                <a href="#results" class="nav-link">Results</a>
                <a href="#demo" class="nav-link">Demo</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden text-gray-700 focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>
        </nav>
        <div id="mobile-menu" class="hidden md:hidden px-6 pt-2 pb-4 space-y-2">
            <a href="#overview" class="block nav-link">Overview</a>
            <a href="#architecture" class="block nav-link">Architecture</a>
            <a href="#data" class="block nav-link">Data</a>
            <a href="#model" class="block nav-link">Model</a>
            <a href="#results" class="block nav-link">Results</a>
            <a href="#demo" class="block nav-link">Demo</a>
        </div>
    </header>

    <main class="container mx-auto px-6 py-12">

        <!-- Section 1: Overview -->
        <section id="overview" class="min-h-[80vh] flex flex-col justify-center">
            <h1 class="text-4xl md:text-5xl font-bold mb-4 text-gray-800 leading-tight">A Multi-Modal System for Real-Time Continuous Sign Language Recognition</h1>
            <p class="text-lg text-gray-600 mb-8 max-w-3xl">
                This project presents a complete, end-to-end system designed to bridge the communication gap for the DHH community. It translates continuous sign language from a webcam into natural, spoken language in real-time, leveraging a sophisticated pipeline of deep learning and NLP models.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
                <div class="stat-card bg-white p-6 rounded-lg shadow-md border-l-4 border-blue-400">
                    <h3 class="text-gray-500 text-sm font-medium">Model Accuracy</h3>
                    <p class="text-3xl font-bold text-gray-800">98.5%</p>
                </div>
                <div class="stat-card bg-white p-6 rounded-lg shadow-md border-l-4 border-green-400">
                    <h3 class="text-gray-500 text-sm font-medium">F1-Score</h3>
                    <p class="text-3xl font-bold text-gray-800">0.978</p>
                </div>
                <div class="stat-card bg-white p-6 rounded-lg shadow-md border-l-4 border-purple-400">
                    <h3 class="text-gray-500 text-sm font-medium">Vocabulary Size</h3>
                    <p class="text-3xl font-bold text-gray-800">56 Signs</p>
                </div>
                <div class="stat-card bg-white p-6 rounded-lg shadow-md border-l-4 border-yellow-400">
                    <h3 class="text-gray-500 text-sm font-medium">Feature Dimensions</h3>
                    <p class="text-3xl font-bold text-gray-800">1,581</p>
                </div>
            </div>
        </section>

        <!-- Section 2: System Architecture -->
        <section id="architecture" class="py-20">
            <h2 class="text-3xl font-bold text-center mb-4">Interactive System Architecture</h2>
            <p class="text-center text-gray-600 mb-12 max-w-3xl mx-auto">
                The system is designed as a modular pipeline, where each component handles a specific task. This separation of concerns ensures scalability and robustness. Click on any block to learn more about its role in the recognition process.
            </p>
            <div class="flex flex-col lg:flex-row items-center justify-center gap-4 mb-8">
                <div id="arch-data" class="arch-block bg-white p-4 rounded-lg shadow-md border-2 border-gray-200 w-full lg:w-48 text-center active">
                    <h3 class="font-semibold">1. Data Acquisition</h3>
                </div>
                <div class="arch-arrow transform lg:rotate-0 rotate-90">&rarr;</div>
                <div id="arch-features" class="arch-block bg-white p-4 rounded-lg shadow-md border-2 border-gray-200 w-full lg:w-48 text-center">
                    <h3 class="font-semibold">2. Feature Extraction</h3>
                </div>
                <div class="arch-arrow transform lg:rotate-0 rotate-90">&rarr;</div>
                <div id="arch-model" class="arch-block bg-white p-4 rounded-lg shadow-md border-2 border-gray-200 w-full lg:w-48 text-center">
                    <h3 class="font-semibold">3. Bi-LSTM Classifier</h3>
                </div>
                 <div class="arch-arrow transform lg:rotate-0 rotate-90">&rarr;</div>
                <div id="arch-inference" class="arch-block bg-white p-4 rounded-lg shadow-md border-2 border-gray-200 w-full lg:w-48 text-center">
                    <h3 class="font-semibold">4. Inference & NLP</h3>
                </div>
            </div>
            <div id="architecture-details" class="bg-white p-8 rounded-lg shadow-inner border border-gray-200 min-h-[200px] transition-all duration-500">
                <!-- Details will be injected here by JS -->
            </div>
        </section>

        <!-- Section 3: Data & Features -->
        <section id="data" class="py-20 bg-gray-50 rounded-lg">
            <h2 class="text-3xl font-bold text-center mb-4">Data & Multi-Modal Features</h2>
            <p class="text-center text-gray-600 mb-12 max-w-3xl mx-auto">
                A high-quality, diverse dataset is the foundation of our model. We captured thousands of video clips and used Google's MediaPipe to extract a rich, multi-modal feature vector from every frame, capturing hand movements, facial expressions, and body posture.
            </p>
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-12 items-center">
                <div>
                    <h3 class="text-xl font-semibold mb-4">Vocabulary Used for Training</h3>
                    <p class="text-gray-600 mb-4">The model was trained on a vocabulary of 56 common signs, including alphabets, numbers, and core conversational words to enable the construction of basic sentences.</p>
                    <div class="bg-white p-4 rounded-lg shadow-md max-h-80 overflow-y-auto">
                        <table class="w-full text-sm text-left text-gray-500">
                            <thead class="text-xs text-gray-700 uppercase bg-gray-100">
                                <tr>
                                    <th scope="col" class="px-6 py-3">Category</th>
                                    <th scope="col" class="px-6 py-3">Words</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="bg-white border-b"><td class="px-6 py-4 font-medium">Greetings & Basics</td><td class="px-6 py-4">Name, What, Nice, You, Meet, Learn, Sign, No, Not, Have</td></tr>
                                <tr class="bg-gray-50 border-b"><td class="px-6 py-4 font-medium">People & Places</td><td class="px-6 py-4">Student, Teacher, Sister, Brother, Girl-friend, Restroom, Classroom</td></tr>
                                <tr class="bg-white border-b"><td class="px-6 py-4 font-medium">Actions & Objects</td><td class="px-6 py-4">Buy, Food</td></tr>
                                <tr class="bg-gray-50 border-b"><td class="px-6 py-4 font-medium">Alphabet</td><td class="px-6 py-4">A, B, C... Z (26 signs)</td></tr>
                                <tr class="bg-white"><td class="px-6 py-4 font-medium">Numbers</td><td class="px-6 py-4">1, 2, 3... 10 (10 signs)</td></tr>
                            </tbody>
                        </table>
                    </div>
                     <p class="text-sm text-gray-500 mt-4">A special "_blank_" class was also included, containing non-sign movements and periods of rest. This is crucial for enabling the system to segment continuous gestures accurately in a live video stream.</p>
                </div>
                <div>
                    <h3 class="text-xl font-semibold mb-4 text-center">Feature Vector Composition (1,581 Dims)</h3>
                    <div class="chart-container">
                        <canvas id="featuresChart"></canvas>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 4: Model & Training -->
        <section id="model" class="py-20">
            <h2 class="text-3xl font-bold text-center mb-4">Bi-LSTM Model & Training</h2>
            <p class="text-center text-gray-600 mb-12 max-w-3xl mx-auto">
               The core of our system is a Bidirectional Long Short-Term Memory (Bi-LSTM) network. This architecture is ideal for understanding sequential data like sign language because it processes information in both forward and reverse time, giving it a deeper contextual understanding of each gesture.
            </p>
            <div class="bg-white p-8 rounded-lg shadow-lg border border-gray-200">
                 <div class="flex justify-center mb-6">
                    <div class="tabs">
                      <button id="acc-btn" class="px-4 py-2 font-semibold text-white bg-blue-500 rounded-l-md focus:outline-none">Accuracy</button>
                      <button id="loss-btn" class="px-4 py-2 font-semibold text-gray-700 bg-gray-200 rounded-r-md focus:outline-none">Loss</button>
                    </div>
                </div>
                <div class="chart-container mx-auto">
                    <canvas id="trainingHistoryChart"></canvas>
                </div>
                 <p class="text-center text-sm text-gray-500 mt-4">The model was trained for 100 epochs using the Adam optimizer. The chart shows the training and validation metrics, demonstrating effective learning without significant overfitting, thanks to regularization techniques like Dropout and Early Stopping.</p>
            </div>
        </section>

        <!-- Section 5: Evaluation & Results -->
        <section id="results" class="py-20 bg-gray-50 rounded-lg">
            <h2 class="text-3xl font-bold text-center mb-4">Evaluation & Results</h2>
            <p class="text-center text-gray-600 mb-12 max-w-3xl mx-auto">
                The model's performance was rigorously evaluated on a held-out test set. We used a confusion matrix to analyze class-specific performance and standard metrics to quantify its overall effectiveness.
            </p>
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-12 items-center">
                <div>
                     <h3 class="text-xl font-semibold mb-4">Illustrative Confusion Matrix</h3>
                     <p class="text-gray-600 mb-4">A confusion matrix helps visualize the model's performance on individual classes. This is an illustrative subset showing performance on commonly confused signs. Hover over a cell to see the details.</p>
                     <div id="confusion-matrix-container" class="bg-white p-4 rounded-lg shadow-md">
                        <!-- Confusion Matrix will be generated by JS -->
                     </div>
                     <p class="text-xs text-gray-500 mt-2">The main diagonal (top-left to bottom-right) shows correct predictions. Off-diagonal cells show misclassifications.</p>
                </div>
                <div>
                    <h3 class="text-xl font-semibold mb-4">Key Performance Metrics</h3>
                    <div class="space-y-4">
                        <div class="bg-white p-4 rounded-lg shadow-sm">
                            <div class="flex justify-between items-center mb-1">
                                <h4 class="font-medium">Accuracy</h4>
                                <span class="font-bold text-blue-600">98.5%</span>
                            </div>
                            <div class="w-full bg-gray-200 rounded-full h-2.5"><div class="bg-blue-600 h-2.5 rounded-full" style="width: 98.5%"></div></div>
                            <p class="text-xs text-gray-500 mt-1">Overall percentage of correct predictions.</p>
                        </div>
                         <div class="bg-white p-4 rounded-lg shadow-sm">
                            <div class="flex justify-between items-center mb-1">
                                <h4 class="font-medium">Precision</h4>
                                <span class="font-bold text-green-600">97.9%</span>
                            </div>
                            <div class="w-full bg-gray-200 rounded-full h-2.5"><div class="bg-green-600 h-2.5 rounded-full" style="width: 97.9%"></div></div>
                             <p class="text-xs text-gray-500 mt-1">Of all positive predictions, how many were truly positive.</p>
                        </div>
                        <div class="bg-white p-4 rounded-lg shadow-sm">
                            <div class="flex justify-between items-center mb-1">
                                <h4 class="font-medium">Recall</h4>
                                <span class="font-bold text-purple-600">97.8%</span>
                            </div>
                            <div class="w-full bg-gray-200 rounded-full h-2.5"><div class="bg-purple-600 h-2.5 rounded-full" style="width: 97.8%"></div></div>
                            <p class="text-xs text-gray-500 mt-1">Of all actual positives, how many were identified.</p>
                        </div>
                         <div class="bg-white p-4 rounded-lg shadow-sm">
                            <div class="flex justify-between items-center mb-1">
                                <h4 class="font-medium">F1-Score</h4>
                                <span class="font-bold text-yellow-600">97.8%</span>
                            </div>
                            <div class="w-full bg-gray-200 rounded-full h-2.5"><div class="bg-yellow-500 h-2.5 rounded-full" style="width: 97.8%"></div></div>
                            <p class="text-xs text-gray-500 mt-1">The harmonic mean of Precision and Recall.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- Section 6: Simulated Demo -->
        <section id="demo" class="py-20">
            <h2 class="text-3xl font-bold text-center mb-4">End-to-End Simulation</h2>
            <p class="text-center text-gray-600 mb-8 max-w-3xl mx-auto">
                This simulation demonstrates the full pipeline in action. Click "Run Simulation" to see how a sequence of signed gestures is processed from raw recognition to a final, grammatically refined sentence.
            </p>
            <div class="bg-white rounded-lg shadow-xl p-8 max-w-4xl mx-auto border border-gray-200">
                <div class="flex justify-center mb-6">
                    <button id="run-simulation-btn" class="px-6 py-3 bg-blue-500 text-white font-semibold rounded-lg shadow-md hover:bg-blue-600 focus:outline-none focus:ring-2 focus:ring-blue-400 focus:ring-opacity-75 transition-transform transform hover:scale-105">
                        Run Simulation
                    </button>
                </div>
                <div class="bg-gray-800 rounded-lg p-6 font-mono text-white min-h-[200px] flex flex-col justify-center">
                    <div id="simulation-output" class="space-y-2">
                        <p class="text-gray-400">&gt; Awaiting simulation...</p>
                    </div>
                </div>
            </div>
        </section>

    </main>
    
    <script>
        document.addEventListener('DOMContentLoaded', () => {

            // Mobile menu toggle
            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileMenu = document.getElementById('mobile-menu');
            mobileMenuButton.addEventListener('click', () => {
                mobileMenu.classList.toggle('hidden');
            });

            // Navigation scroll highlighting
            const sections = document.querySelectorAll('section');
            const navLinks = document.querySelectorAll('.nav-link');

            const observerOptions = {
                root: null,
                rootMargin: '0px',
                threshold: 0.5
            };

            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href').substring(1) === entry.target.id) {
                                link.classList.add('active');
                            }
                        });
                    }
                });
            }, observerOptions);

            sections.forEach(section => {
                observer.observe(section);
            });
            
            // Architecture Diagram Interactivity
            const archBlocks = document.querySelectorAll('.arch-block');
            const archDetailsContainer = document.getElementById('architecture-details');
            const archDetailsContent = {
                'arch-data': {
                    title: '1. Data Acquisition Subsystem',
                    text: 'The process begins with a custom Python script to record multiple, fixed-duration (10-second) videos for each gesture class (e.g., "hello", "_blank_"). This creates a structured, labeled dataset of raw MP4 videos, which serves as the ground truth for our model.'
                },
                'arch-features': {
                    title: '2. Multi-Modal Feature Extraction',
                    text: 'Using Google\'s MediaPipe, we process the raw videos to extract 1,581 keypoints per frame across three concurrent models: Face Mesh (468 landmarks), Pose (17 upper-body landmarks), and Hands (21 landmarks per hand). This data is normalized to ensure robustness and concatenated into a comprehensive feature vector.'
                },
                'arch-model': {
                    title: '3. Bidirectional LSTM Classifier',
                    text: 'The feature vectors are fed into a Bidirectional LSTM (Bi-LSTM) network. This model is specifically designed to learn temporal patterns by processing data in both forward and reverse time, allowing it to better understand the context of a gesture within a sequence. It outputs a probability for each possible sign.'
                },
                'arch-inference': {
                    title: '4. Real-Time Inference & NLP Refinement',
                    text: 'The trained model is deployed in a high-performance gRPC service. A sliding window algorithm continuously analyzes the incoming stream of keypoints. The raw recognized signs (e.g., "you name what") are then passed to an NLP service (using Gemini) to be rephrased into a grammatically correct sentence (e.g., "What is your name?").'
                }
            };

            function updateArchDetails(id) {
                const content = archDetailsContent[id];
                archDetailsContainer.innerHTML = `<h3 class="text-xl font-bold mb-2 text-gray-800">${content.title}</h3><p class="text-gray-600 leading-relaxed">${content.text}</p>`;
            }

            archBlocks.forEach(block => {
                block.addEventListener('click', () => {
                    archBlocks.forEach(b => b.classList.remove('active'));
                    block.classList.add('active');
                    updateArchDetails(block.id);
                });
            });

            updateArchDetails('arch-data'); // Initial content

            // Features Chart (Donut)
            const featuresCtx = document.getElementById('featuresChart').getContext('2d');
            new Chart(featuresCtx, {
                type: 'doughnut',
                data: {
                    labels: ['Face Mesh Features', 'Hand Features', 'Pose Features'],
                    datasets: [{
                        label: 'Feature Count',
                        data: [1404, 126, 51],
                        backgroundColor: [
                            'rgba(66, 153, 225, 0.8)',
                            'rgba(74, 222, 128, 0.8)',
                            'rgba(159, 122, 234, 0.8)',
                        ],
                        borderColor: [
                            'rgba(66, 153, 225, 1)',
                            'rgba(74, 222, 128, 1)',
                            'rgba(159, 122, 234, 1)',
                        ],
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: { position: 'top' },
                        title: { display: false },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    let label = context.label || '';
                                    if (label) {
                                        label += ': ';
                                    }
                                    if (context.parsed !== null) {
                                        label += context.parsed;
                                    }
                                    return label;
                                }
                            }
                        }
                    }
                }
            });
            
            // Training History Chart (Line)
            const trainingCtx = document.getElementById('trainingHistoryChart').getContext('2d');
            const epochs = Array.from({length: 100}, (_, i) => i + 1);
            const trainAcc = epochs.map(e => 0.6 + 0.38 * (1 - Math.exp(-e/25)) + (Math.random()-0.5)*0.02);
            const valAcc = epochs.map(e => 0.62 + 0.365 * (1 - Math.exp(-e/25)) + (Math.random()-0.5)*0.015);
            const trainLoss = epochs.map(e => 1.5 * Math.exp(-e/20) + 0.05 + (Math.random()-0.5)*0.03);
            const valLoss = epochs.map(e => 1.4 * Math.exp(-e/20) + 0.075 + (Math.random()-0.5)*0.02);
            
            const trainingChartConfig = {
                type: 'line',
                data: {
                    labels: epochs,
                    datasets: []
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: { title: { display: true, text: 'Epoch' } },
                        y: { title: { display: true, text: 'Value' } }
                    },
                    plugins: { legend: { position: 'bottom' } }
                }
            };
            
            const trainingHistoryChart = new Chart(trainingCtx, trainingChartConfig);

            function updateTrainingChart(metric) {
                const accBtn = document.getElementById('acc-btn');
                const lossBtn = document.getElementById('loss-btn');
                
                if (metric === 'accuracy') {
                    accBtn.classList.add('bg-blue-500', 'text-white');
                    accBtn.classList.remove('bg-gray-200', 'text-gray-700');
                    lossBtn.classList.add('bg-gray-200', 'text-gray-700');
                    lossBtn.classList.remove('bg-blue-500', 'text-white');
                    
                    trainingHistoryChart.data.datasets = [
                        { label: 'Training Accuracy', data: trainAcc, borderColor: 'rgba(66, 153, 225, 1)', backgroundColor: 'rgba(66, 153, 225, 0.2)', tension: 0.1, fill: true },
                        { label: 'Validation Accuracy', data: valAcc, borderColor: 'rgba(237, 137, 54, 1)', backgroundColor: 'rgba(237, 137, 54, 0.2)', tension: 0.1, fill: true }
                    ];
                    trainingHistoryChart.options.scales.y.title.text = 'Accuracy';
                } else { // loss
                    lossBtn.classList.add('bg-blue-500', 'text-white');
                    lossBtn.classList.remove('bg-gray-200', 'text-gray-700');
                    accBtn.classList.add('bg-gray-200', 'text-gray-700');
                    accBtn.classList.remove('bg-blue-500', 'text-white');
                    
                    trainingHistoryChart.data.datasets = [
                        { label: 'Training Loss', data: trainLoss, borderColor: 'rgba(239, 68, 68, 1)', backgroundColor: 'rgba(239, 68, 68, 0.2)', tension: 0.1, fill: true },
                        { label: 'Validation Loss', data: valLoss, borderColor: 'rgba(16, 185, 129, 1)', backgroundColor: 'rgba(16, 185, 129, 0.2)', tension: 0.1, fill: true }
                    ];
                    trainingHistoryChart.options.scales.y.title.text = 'Loss';
                }
                trainingHistoryChart.update();
            }

            document.getElementById('acc-btn').addEventListener('click', () => updateTrainingChart('accuracy'));
            document.getElementById('loss-btn').addEventListener('click', () => updateTrainingChart('loss'));
            updateTrainingChart('accuracy');

            // Confusion Matrix
            const cmContainer = document.getElementById('confusion-matrix-container');
            const cmData = [
                [95, 2, 1, 0, 2],
                [3, 94, 2, 1, 0],
                [0, 1, 98, 1, 0],
                [1, 0, 2, 96, 1],
                [2, 1, 0, 2, 95]
            ];
            const cmLabels = ['Brother', 'Sister', 'Teacher', 'Student', 'Buy'];
            let cmHtml = '<div class="grid grid-cols-6 gap-1 text-center font-medium text-xs">';
            cmHtml += '<div></div>'; // Top-left empty cell
            cmLabels.forEach(label => cmHtml += `<div class="p-2 bg-gray-100 rounded-t-md">Predicted<br>${label}</div>`);
            
            cmData.forEach((row, i) => {
                cmHtml += `<div class="p-2 bg-gray-100 rounded-l-md self-center">Actual<br>${cmLabels[i]}</div>`;
                row.forEach((val, j) => {
                    const isDiagonal = i === j;
                    const bgColor = isDiagonal ? `bg-green-100` : `bg-red-100`;
                    const textColor = isDiagonal ? `text-green-800` : `text-red-800`;
                    const tooltip = isDiagonal 
                        ? `${val} instances of '${cmLabels[i]}' were correctly classified.`
                        : `${val} instances of '${cmLabels[i]}' were incorrectly classified as '${cmLabels[j]}'.`;
                    cmHtml += `<div class="${bgColor} ${textColor} p-4 rounded-md flex items-center justify-center relative group" title="${tooltip}">
                                  ${val}
                               </div>`;
                });
            });
            cmHtml += '</div>';
            cmContainer.innerHTML = cmHtml;

            // Simulation
            const simBtn = document.getElementById('run-simulation-btn');
            const simOutput = document.getElementById('simulation-output');
            
            simBtn.addEventListener('click', () => {
                simBtn.disabled = true;
                simBtn.classList.add('opacity-50', 'cursor-not-allowed');
                simOutput.innerHTML = '';

                const steps = [
                    { text: 'Starting simulation...', delay: 500, style: 'text-gray-400' },
                    { text: 'User performs "YOU" sign...', delay: 1000, style: 'text-yellow-300' },
                    { text: 'Bi-LSTM recognized: <strong>YOU</strong> (Confidence: 99.2%)', delay: 1500, style: 'text-green-300' },
                    { text: 'User performs "NAME" sign...', delay: 1000, style: 'text-yellow-300' },
                    { text: 'Bi-LSTM recognized: <strong>NAME</strong> (Confidence: 98.8%)', delay: 1500, style: 'text-green-300' },
                    { text: 'User performs "WHAT" sign...', delay: 1000, style: 'text-yellow-300' },
                    { text: 'Bi-LSTM recognized: <strong>WHAT</strong> (Confidence: 99.5%)', delay: 1500, style: 'text-green-300' },
                    { text: 'Raw sequence: ["YOU", "NAME", "WHAT"]', delay: 1000, style: 'text-cyan-300' },
                    { text: 'Sending to NLP Refinement Service (Gemini)...', delay: 1500, style: 'text-purple-300' },
                    { text: 'Final Translation: <strong>What is your name?</strong>', delay: 2000, style: 'text-white text-lg' },
                    { text: 'Simulation complete.', delay: 1000, style: 'text-gray-400' }
                ];

                let cumulativeDelay = 0;
                steps.forEach(step => {
                    cumulativeDelay += step.delay;
                    setTimeout(() => {
                        const p = document.createElement('p');
                        p.className = step.style;
                        p.innerHTML = `&gt; ${step.text}`;
                        simOutput.appendChild(p);
                        simOutput.scrollTop = simOutput.scrollHeight;
                    }, cumulativeDelay);
                });

                setTimeout(() => {
                     simBtn.disabled = false;
                     simBtn.classList.remove('opacity-50', 'cursor-not-allowed');
                }, cumulativeDelay + 500);
            });

        });
    </script>
</body>
</html>
